{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/kaggle/input/collection-of-textures-in-colorectal-cancer/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importing required libraries\n\nimport os\nimport random\nimport time\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets\nfrom torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\nfrom transformers import BeitModel\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import cycle\nfrom pathlib import Path\nfrom typing import Tuple, Dict, List\nfrom pathlib import Path\nfrom typing import Tuple, Dict, List\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport logging\nimport cv2\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor, as_completed","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\nORIGINAL_BASE = Path(\"/kaggle/input/collection-of-textures-in-colorectal-cancer/Kather_texture_2016_image_tiles_5000\")\nWORKING_BASE = Path(\"/kaggle/working/Kather_texture_working\")       \nPROCESSED_BASE = Path(\"/kaggle/working/Kather_texture_processed\")   \nSPLIT_BASE     = Path(\"/kaggle/working/Kather_texture_split\")     \n\nLABEL_MAP = {\n    \"01_TUMOR\": \"Tumor\",\n    \"02_STROMA\": \"Stroma\",\n    \"03_COMPLEX\": \"Complex\",\n    \"04_LYMPHO\": \"Lympho\",\n    \"05_DEBRIS\": \"Debris\",\n    \"06_MUCOSA\": \"Mucosa\",\n    \"07_ADIPOSE\": \"Adipose\",\n    \"08_EMPTY\": \"Empty\"\n}\n\nBEST_DROPOUT = 0.3           \nBEST_LR = 1e-4              \nLABEL_SMOOTHING = 0.1        \nWEIGHT_DECAY = 1e-5          \nEPOCHS = 20                  \nPATIENCE = 5                \nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIMG_SIZE = 224\nBLUR_KERNEL = (5, 5)\nPREPROCESS_WORKERS = min(8, os.cpu_count() or 4)\nBATCH_SIZE = 64\nNUM_WORKERS = 4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# training hparams\nEPOCHS = 100\nPATIENCE = 4\n\n# logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\nlog = logging.getLogger(\"kather_pipeline\")\n\ndef preprocess_image(in_path: Path, out_path: Path, img_size: int = IMG_SIZE) -> Tuple[Path, bool, str]:\n    try:\n        img = cv2.imread(str(in_path))  # BGR\n        if img is None:\n            return in_path, False, \"unreadable\"\n\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Per-channel histogram equalization\n        r, g, b = cv2.split(img)\n        r = cv2.equalizeHist(r)\n        g = cv2.equalizeHist(g)\n        b = cv2.equalizeHist(b)\n        img = cv2.merge([r, g, b])\n\n        # Blur & resize\n        img = cv2.GaussianBlur(img, BLUR_KERNEL, 0)\n        img = cv2.resize(img, (img_size, img_size), interpolation=cv2.INTER_AREA)\n\n        out_path.parent.mkdir(parents=True, exist_ok=True)\n        saved = cv2.imwrite(str(out_path), cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n        return out_path, bool(saved), \"ok\" if saved else \"save_failed\"\n    except Exception as e:\n        return in_path, False, f\"error:{e}\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_folder(src_folder: Path, dst_folder: Path, label: str, limit_per_class: int | None = None) -> int:\n    files = sorted(src_folder.glob(\"*.tif\"))\n    if not files:\n        log.warning(\"No .tif images found in %s\", src_folder)\n        return 0\n    if limit_per_class:\n        files = files[:limit_per_class]\n\n    dst_folder.mkdir(parents=True, exist_ok=True)\n    futures, ok_count = [], 0\n    with ThreadPoolExecutor(max_workers=PREPROCESS_WORKERS) as ex:\n        for idx, img_path in enumerate(files):\n            new_name = f\"{label}_{idx:05d}.tif\"\n            out_path = dst_folder / new_name\n            futures.append(ex.submit(preprocess_image, img_path, out_path, IMG_SIZE))\n        for fut in tqdm(as_completed(futures), total=len(futures), desc=f\"Preprocessing {label}\"):\n            _, ok, _ = fut.result()\n            ok_count += int(ok)\n\n    log.info(\"Processed %d/%d for %s -> %s\", ok_count, len(files), label, dst_folder)\n    return ok_count","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_all(src_base: Path, dst_base: Path, labels: Dict[str, str], limit_per_class: int | None = None):\n    total = 0\n    for new_label in labels.values():\n        src_folder = src_base / new_label\n        dst_folder = dst_base / new_label\n        if src_folder.exists():\n            total += preprocess_folder(src_folder, dst_folder, new_label, limit_per_class=limit_per_class)\n        else:\n            log.warning(\"Missing folder: %s\", src_folder)\n    log.info(\"Total processed images: %d\", total)\n    show_tree(dst_base, depth=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_samples(base: Path, labels: List[str], samples_per_class: int = 3):\n    log.info(\"Displaying sample images...\")\n    n_rows = len(labels)\n    n_cols = samples_per_class\n    fig, axs = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 3 * n_rows))\n    if n_rows == 1:\n        axs = np.array([axs])\n    for i, label in enumerate(sorted(labels)):\n        folder = base / label\n        imgs = list(folder.glob(\"*.tif\"))\n        if not imgs:\n            continue\n        sample = random.sample(imgs, min(samples_per_class, len(imgs)))\n        for j in range(n_cols):\n            ax = axs[i, j]\n            if j < len(sample):\n                img = cv2.imread(str(sample[j]))\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                ax.imshow(img)\n                ax.set_title(f\"{label}\\n{sample[j].name}\", fontsize=9)\n            ax.axis(\"off\")\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Loading\n\ndef ensure_rgb(img):\n    if img.mode != 'RGB':\n        img = img.convert('RGB')\n    return img\n\ndef load_data(data_path, batch_size=32):\n    dataset = datasets.ImageFolder(root=data_path, transform=None)\n    labels = np.array([label for _, label in dataset.imgs])\n\n    # Split train/val/test\n    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n    train_val_idx, test_idx = next(sss1.split(np.zeros(len(labels)), labels))\n\n    train_val_labels = labels[train_val_idx]\n    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.1765, random_state=42)\n    train_idx, val_idx = next(sss2.split(np.zeros(len(train_val_labels)), train_val_labels))\n\n    train_idx = train_val_idx[train_idx]\n    val_idx = train_val_idx[val_idx]\n\n    train_data = Subset(dataset, train_idx)\n    val_data = Subset(dataset, val_idx)\n    test_data = Subset(dataset, test_idx)\n\n    # Transforms\n    train_transform = transforms.Compose([\n        transforms.Lambda(ensure_rgb),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ])\n    val_transform = train_transform\n\n    train_data.dataset.transform = train_transform\n    val_data.dataset.transform = val_transform\n    test_data.dataset.transform = val_transform\n\n    # Loaders\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    return dataset.classes, train_loader, val_loader, test_loader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Definition\n\nclass HybridBeitEfficientNet(nn.Module):\n    def __init__(self, num_classes, dropout=0.3, fc_units=512):\n        super(HybridBeitEfficientNet, self).__init__()\n\n        self.beit_backbone = BeitModel.from_pretrained('microsoft/beit-base-patch16-224-pt22k')\n        for param in self.beit_backbone.parameters():\n            param.requires_grad = False\n\n        weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n        self.efficientnet_backbone = efficientnet_b0(weights=weights)\n        self.efficientnet_backbone.classifier = nn.Identity()\n        for param in self.efficientnet_backbone.parameters():\n            param.requires_grad = False\n\n        beit_feature_dim = self.beit_backbone.config.hidden_size\n        efficientnet_feature_dim = 1280\n        combined_feature_dim = beit_feature_dim + efficientnet_feature_dim\n\n        self.fc = nn.Sequential(\n            nn.Linear(combined_feature_dim, fc_units),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(fc_units, num_classes)\n        )\n\n    def forward(self, x):\n        beit_outputs = self.beit_backbone(pixel_values=x).last_hidden_state\n        beit_cls = beit_outputs[:, 0, :]\n        efficientnet_out = self.efficientnet_backbone(x)\n        combined = torch.cat((beit_cls, efficientnet_out), dim=1)\n        return self.fc(combined)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10, patience=4, model_name=\"model\"):\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n    best_val_acc = -1.0\n    epochs_no_improve = 0\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,\n                                                           patience=int(patience/2), verbose=True)\n    best_model_path = f'{model_name}_best_model.pth'\n    best_model_wts = model.state_dict()\n\n    for epoch in range(num_epochs):\n        # Train\n        model.train()\n        running_loss, correct_train, total_train = 0.0, 0, 0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total_train += labels.size(0)\n            correct_train += (predicted == labels).sum().item()\n\n        train_accuracy = correct_train / total_train\n        avg_train_loss = running_loss / len(train_loader)\n\n        # Validation\n        model.eval()\n        val_loss, correct_val, total_val = 0.0, 0, 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total_val += labels.size(0)\n                correct_val += (predicted == labels).sum().item()\n\n        val_accuracy = correct_val / total_val\n        avg_val_loss = val_loss / len(val_loader)\n\n        history['train_loss'].append(avg_train_loss)\n        history['train_acc'].append(train_accuracy)\n        history['val_loss'].append(avg_val_loss)\n        history['val_acc'].append(val_accuracy)\n        history['lr'].append(optimizer.param_groups[0]['lr'])\n\n        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | \"\n              f\"Train Acc: {train_accuracy*100:.2f}% | Val Loss: {avg_val_loss:.4f} | \"\n              f\"Val Acc: {val_accuracy*100:.2f}% | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n\n        scheduler.step(avg_val_loss)\n\n        if val_accuracy > best_val_acc:\n            best_val_acc = val_accuracy\n            best_model_wts = model.state_dict()\n            torch.save(best_model_wts, best_model_path)\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    model.load_state_dict(best_model_wts)\n    return model, history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation\n\ndef evaluate_model(model, test_loader, criterion, device, class_names, model_name=\"Model\"):\n    model.eval()\n    all_labels, all_predictions = [], []\n    total_loss = 0.0\n    start_time = time.time()\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n    end_time = time.time()\n    accuracy = accuracy_score(all_labels, all_predictions)\n    avg_loss = total_loss / len(test_loader.dataset)\n    sensitivity = recall_score(all_labels, all_predictions, average='weighted')\n    precision = precision_score(all_labels, all_predictions, average='weighted')\n    f1 = f1_score(all_labels, all_predictions, average='weighted')\n    report_dict = classification_report(all_labels, all_predictions, target_names=class_names, output_dict=True)\n    per_class_accuracy = [report_dict[name]['recall'] for name in class_names]\n    std_dev = np.std(per_class_accuracy)\n\n    print(f\"Accuracy: {accuracy:.4f} | Loss: {avg_loss:.4f} | Sensitivity: {sensitivity:.4f} | \"\n          f\"Precision: {precision:.4f} | F1: {f1:.4f} | Time: {end_time-start_time:.2f}s\")\n\n    cm = confusion_matrix(all_labels, all_predictions)\n    plt.figure(figsize=(len(class_names)+2, len(class_names)+2))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title(f'Confusion Matrix - {model_name}')\n    plt.show()\n\n    return {\n        'accuracy': accuracy, 'loss': avg_loss, 'sensitivity': sensitivity,\n        'precision': precision, 'f1_score': f1, 'std_dev_accuracy_per_class': std_dev\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting\n\ndef plot_training_curves(history, model_name=\"Model\"):\n    epochs = range(1, len(history['train_loss']) + 1)\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, history['train_loss'], label='Training Loss', marker='o')\n    plt.plot(epochs, history['val_loss'], label='Validation Loss', marker='o')\n    plt.legend(), plt.grid(True)\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, history['train_acc'], label='Training Accuracy', marker='o')\n    plt.plot(epochs, history['val_acc'], label='Validation Accuracy', marker='o')\n    plt.legend(), plt.grid(True)\n    plt.show()\n\ndef plot_multiclass_roc_auc(model, test_loader, class_names, device, model_name=\"Model\"):\n    model.eval()\n    all_labels, all_probs = [], []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            probs = F.softmax(outputs, dim=1).cpu().numpy()\n            all_probs.extend(probs)\n            all_labels.extend(labels.cpu().numpy())\n    all_labels = np.array(all_labels)\n    y_true_bin = label_binarize(all_labels, classes=list(range(len(class_names))))\n    fpr, tpr, roc_auc = {}, {}, {}\n    for i in range(len(class_names)):\n        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], np.array(all_probs)[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    plt.figure(figsize=(8, 6))\n    colors = cycle(plt.cm.tab10.colors)\n    for i, color in zip(range(len(class_names)), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n    plt.legend(), plt.grid(True)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Subset\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nimport random, time, copy\n\n\ndef run_pso_optimization(train_loader, val_loader, test_loader, class_names, device):\n    import copy, time, random\n    # ---- PSO (your original structure) ----\n    class PSO:\n        def __init__(self, fitness_func, bounds, num_particles, max_iter, c1=2.0, c2=2.0, w=0.7):\n            self.fitness_func = fitness_func\n            if isinstance(bounds, dict):\n                self.hp_keys = list(bounds.keys())\n                self.bounds = [bounds[k] for k in self.hp_keys]\n            else:\n                self.bounds = bounds\n                self.hp_keys = [f'HP_{i}' for i in range(len(bounds))]\n            self.num_particles = num_particles\n            self.max_iter = max_iter\n            self.c1, self.c2, self.w = c1, c2, w\n            self.dim = len(self.bounds)\n            self.minb = np.array([b[0] for b in self.bounds])\n            self.maxb = np.array([b[1] for b in self.bounds])\n            self.pos = np.random.uniform(self.minb, self.maxb, (self.num_particles, self.dim))\n            self.vel = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n            self.pbest_pos = self.pos.copy()\n            self.pbest_score = np.full(self.num_particles, -np.inf)\n            self.gbest_pos = None\n            self.gbest_score = -np.inf\n            self.best_hps = None\n\n        def _eval(self, hp_dict, tr_loader, va_loader, num_classes, device):\n            return self.fitness_func(hp_dict, tr_loader, va_loader, num_classes, device)\n\n        def optimize(self, tr_loader, va_loader, num_classes, device, patience=7, min_delta=0.005):\n            best_hist, epochs_no_improve = [], 0\n            self.gbest_score, self.gbest_pos, self.best_hps = -np.inf, None, None\n            print(f\"\\n--- Starting PSO: particles={self.num_particles}, iters={self.max_iter} ---\")\n            for it in range(self.max_iter):\n                t0 = time.time()\n                for i in range(self.num_particles):\n                    hp = {\n                        self.hp_keys[0]: self.pos[i, 0],\n                        self.hp_keys[1]: self.pos[i, 1],\n                        self.hp_keys[2]: self.pos[i, 2],\n                        self.hp_keys[3]: int(self.pos[i, 3])\n                    }\n                    s = self._eval(hp, tr_loader, va_loader, num_classes, device)\n                    if s > self.pbest_score[i]:\n                        self.pbest_score[i] = s\n                        self.pbest_pos[i] = self.pos[i].copy()\n                    if s > self.gbest_score:\n                        self.gbest_score = s\n                        self.gbest_pos = self.pos[i].copy()\n                        self.best_hps = copy.deepcopy(hp)\n\n                for i in range(self.num_particles):\n                    r1 = np.random.rand(self.dim); r2 = np.random.rand(self.dim)\n                    cog = self.c1 * r1 * (self.pbest_pos[i] - self.pos[i])\n                    soc = self.c2 * r2 * (self.gbest_pos    - self.pos[i])\n                    self.vel[i] = self.w * self.vel[i] + cog + soc\n                    max_step = (self.maxb - self.minb) * 0.1\n                    self.vel[i] = np.clip(self.vel[i], -max_step, max_step)\n                    self.pos[i] = np.clip(self.pos[i] + self.vel[i], self.minb, self.maxb)\n\n                dt = time.time() - t0\n                print(f\"PSO Iter {it+1}/{self.max_iter} | {dt:.2f}s | gbest={self.gbest_score:.4f} | hps={self.best_hps}\")\n                if not best_hist or (self.gbest_score - max(best_hist) > min_delta):\n                    best_hist.append(self.gbest_score); epochs_no_improve = 0\n                else:\n                    epochs_no_improve += 1\n                if epochs_no_improve >= patience:\n                    print(\"Early stopping PSO.\"); break\n\n            if self.best_hps is None:\n                # fallback\n                return {\n                    self.hp_keys[0]: self.minb[0],\n                    self.hp_keys[1]: self.minb[1],\n                    self.hp_keys[2]: self.minb[2],\n                    self.hp_keys[3]: int(self.minb[3]),\n                }, 0.0\n            return self.best_hps, self.gbest_score\n\n    # fitness uses your HybridBeitEffNet; FC_Units is ignored (head is fixed at 512)\n    def fitness_function_for_pso(hps, tr_loader, va_loader, num_classes, device):\n        lr, wd = hps['LR'], hps['WD']\n        dropout = hps['Dropout']\n        model = HybridBeitEffNet(dropout=dropout, num_classes=num_classes).to(device)\n        for p in model.beit_backbone.parameters(): p.requires_grad = False\n        for p in model.efficientnet_backbone.parameters(): p.requires_grad = False\n        crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n        opt  = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n        model.train()\n        for _ in range(7):\n            for x, y in tr_loader:\n                x, y = x.to(device), y.to(device)\n                opt.zero_grad()\n                loss = crit(model(x), y)\n                loss.backward(); opt.step()\n        # val accuracy\n        model.eval()\n        correct = total = 0\n        with torch.no_grad():\n            for x, y in va_loader:\n                x, y = x.to(device), y.to(device)\n                pred = model(x).argmax(1)\n                total += y.size(0)\n                correct += (pred == y).sum().item()\n        return correct / total\n\n    # bounds (FC_Units present but unused by model; OK to keep)\n    hp_bounds = {'LR': (1e-5, 1e-3), 'WD': (1e-5, 1e-3), 'Dropout': (0.1, 0.5), 'FC_Units': (256, 1024)}\n\n    # Create subset loaders from the original loaders' datasets\n    base_train_ds = train_loader.dataset\n    base_val_ds   = val_loader.dataset\n    frac = 0.5\n    pso_train_idx = random.sample(range(len(base_train_ds)), int(len(base_train_ds)*frac))\n    pso_val_idx   = random.sample(range(len(base_val_ds)),   int(len(base_val_ds)*frac))\n    pso_train_loader = DataLoader(Subset(base_train_ds, pso_train_idx), batch_size=32, shuffle=True,  num_workers=4, pin_memory=True)\n    pso_val_loader   = DataLoader(Subset(base_val_ds,   pso_val_idx),   batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n\n    # optimize\n    start = time.time()\n    opt = PSO(fitness_func=fitness_function_for_pso, bounds=hp_bounds, num_particles=15, max_iter=30, c1=2.0, c2=2.0, w=0.7)\n    best_hps, best_score = opt.optimize(pso_train_loader, pso_val_loader, num_classes=len(class_names), device=device, patience=7, min_delta=0.005)\n    print(f\"\\n--- PSO Finished in {time.time() - start:.2f}s ---\")\n    print(\"Best PSO HPs:\", best_hps, \" | Best score:\", best_score)\n\n    # final train (full loaders)\n    pso_model = HybridBeitEffNet(dropout=best_hps['Dropout'], num_classes=len(class_names)).to(device)\n    for p in pso_model.parameters(): p.requires_grad = True\n    crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n    opti = optim.AdamW(pso_model.parameters(), lr=best_hps['LR']/10, weight_decay=best_hps['WD'])\n    train_model(pso_model, train_loader, val_loader, crit, opti,\n            device=device, num_epochs=50, patience=PATIENCE, model_name='pso_model')\n    torch.save(pso_model.state_dict(), 'pso_model.pth')\n    pso_model.load_state_dict(torch.load('pso_model.pth', map_location=device))\n    pso_metrics = evaluate_model(pso_model, test_loader, crit, device, class_names, model_name=\"PSO Model\")\n    return best_hps, best_score, pso_metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Subset\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom typing import Dict\nimport copy, time, random\n\ndef run_woa_optimization(train_loader, val_loader, test_loader, class_names, device):\n    import copy, time, random\n    # reuse the same class body as PSO (your code), just rename to WOA for now\n    class WOA:\n        def __init__(self, fitness_func, bounds, num_particles, max_iter, c1=2.0, c2=2.0, w=0.7):\n            self.fitness_func = fitness_func\n            if isinstance(bounds, dict):\n                self.hp_keys = list(bounds.keys())\n                self.bounds = [bounds[k] for k in self.hp_keys]\n            else:\n                self.bounds = bounds\n                self.hp_keys = [f'HP_{i}' for i in range(len(bounds))]\n            self.num_particles = num_particles\n            self.max_iter = max_iter\n            self.c1, self.c2, self.w = c1, c2, w\n            self.dim = len(self.bounds)\n            self.minb = np.array([b[0] for b in self.bounds])\n            self.maxb = np.array([b[1] for b in self.bounds])\n            self.pos = np.random.uniform(self.minb, self.maxb, (self.num_particles, self.dim))\n            self.vel = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n            self.pbest_pos = self.pos.copy()\n            self.pbest_score = np.full(self.num_particles, -np.inf)\n            self.gbest_pos = None\n            self.gbest_score = -np.inf\n            self.best_hps = None\n        def _eval(self, hp_dict, tr_loader, va_loader, num_classes, device):\n            return self.fitness_func(hp_dict, tr_loader, va_loader, num_classes, device)\n        def optimize(self, tr_loader, va_loader, num_classes, device, patience=7, min_delta=0.005):\n            best_hist, epochs_no_improve = [], 0\n            self.gbest_score, self.gbest_pos, self.best_hps = -np.inf, None, None\n            print(f\"\\n--- Starting WOA: particles={self.num_particles}, iters={self.max_iter} ---\")\n            for it in range(self.max_iter):\n                t0 = time.time()\n                for i in range(self.num_particles):\n                    hp = {\n                        self.hp_keys[0]: self.pos[i, 0],\n                        self.hp_keys[1]: self.pos[i, 1],\n                        self.hp_keys[2]: self.pos[i, 2],\n                        self.hp_keys[3]: int(self.pos[i, 3])\n                    }\n                    s = self._eval(hp, tr_loader, va_loader, num_classes, device)\n                    if s > self.pbest_score[i]:\n                        self.pbest_score[i] = s\n                        self.pbest_pos[i] = self.pos[i].copy()\n                    if s > self.gbest_score:\n                        self.gbest_score = s\n                        self.gbest_pos = self.pos[i].copy()\n                        self.best_hps = copy.deepcopy(hp)\n                for i in range(self.num_particles):\n                    r1 = np.random.rand(self.dim); r2 = np.random.rand(self.dim)\n                    cog = self.c1 * r1 * (self.pbest_pos[i] - self.pos[i])\n                    soc = self.c2 * r2 * (self.gbest_pos    - self.pos[i])\n                    self.vel[i] = self.w * self.vel[i] + cog + soc\n                    max_step = (self.maxb - self.minb) * 0.1\n                    self.vel[i] = np.clip(self.vel[i], -max_step, max_step)\n                    self.pos[i] = np.clip(self.pos[i] + self.vel[i], self.minb, self.maxb)\n                print(f\"WOA Iter {it+1}/{self.max_iter} | gbest={self.gbest_score:.4f} | hps={self.best_hps}\")\n                if not best_hist or (self.gbest_score - max(best_hist) > min_delta):\n                    best_hist.append(self.gbest_score); epochs_no_improve = 0\n                else:\n                    epochs_no_improve += 1\n                if epochs_no_improve >= patience: \n                    print(\"Early stopping WOA.\"); break\n            if self.best_hps is None:\n                return {\n                    self.hp_keys[0]: self.minb[0],\n                    self.hp_keys[1]: self.minb[1],\n                    self.hp_keys[2]: self.minb[2],\n                    self.hp_keys[3]: int(self.minb[3]),\n                }, 0.0\n            return self.best_hps, self.gbest_score\n\n    def fitness_function_for_woa(hps, tr_loader, va_loader, num_classes, device):\n        lr, wd = hps['LR'], hps['WD']\n        dropout = hps['Dropout']\n        model = HybridBeitEfficientNet(dropout=dropout, num_classes=num_classes).to(device)\n        for p in model.beit_backbone.parameters(): p.requires_grad = False\n        for p in model.efficientnet_backbone.parameters(): p.requires_grad = False\n        crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n        opt  = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n        model.train()\n        for _ in range(7):\n            for x, y in tr_loader:\n                x, y = x.to(device), y.to(device)\n                opt.zero_grad(); loss = crit(model(x), y); loss.backward(); opt.step()\n        model.eval()\n        correct = total = 0\n        with torch.no_grad():\n            for x, y in va_loader:\n                x, y = x.to(device), y.to(device)\n                total += y.size(0)\n                correct += (model(x).argmax(1) == y).sum().item()\n        return correct / total\n\n    hp_bounds = {'LR': (1e-5, 1e-3), 'WD': (1e-5, 1e-3), 'Dropout': (0.1, 0.5), 'FC_Units': (256, 1024)}\n\n    base_train_ds = train_loader.dataset\n    base_val_ds   = val_loader.dataset\n    frac = 0.5\n    woa_train_idx = random.sample(range(len(base_train_ds)), int(len(base_train_ds)*frac))\n    woa_val_idx   = random.sample(range(len(base_val_ds)),   int(len(base_val_ds)*frac))\n    woa_train_loader = DataLoader(Subset(base_train_ds, woa_train_idx), batch_size=32, shuffle=True,  num_workers=4, pin_memory=True)\n    woa_val_loader   = DataLoader(Subset(base_val_ds,   woa_val_idx),   batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n\n    start = time.time()\n    opt = WOA(fitness_func=fitness_function_for_woa, bounds=hp_bounds, num_particles=15, max_iter=30, c1=2.0, c2=2.0, w=0.7)\n    best_hps, best_score = opt.optimize(woa_train_loader, woa_val_loader, num_classes=len(class_names), device=device, patience=7, min_delta=0.005)\n    print(f\"\\n--- WOA Finished in {time.time() - start:.2f}s ---\")\n    print(\"Best WOA HPs:\", best_hps, \" | Best score:\", best_score)\n\n    woa_model = HybridBeitEffNet(dropout=best_hps['Dropout'], num_classes=len(class_names)).to(device)\n    for p in woa_model.parameters(): p.requires_grad = True\n    crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n    opti = optim.AdamW(woa_model.parameters(), lr=best_hps['LR']/10, weight_decay=best_hps['WD'])\n    train_model(woa_model, train_loader, val_loader, crit, opti,\n            device=device, num_epochs=50, patience=PATIENCE, model_name='woa_model')\n    torch.save(woa_model.state_dict(), 'woa_model.pth')\n    woa_model.load_state_dict(torch.load('woa_model.pth', map_location=device))\n    woa_metrics = evaluate_model(woa_model, test_loader, crit, device, class_names, model_name=\"WOA Model\")\n    return best_hps, best_score, woa_metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    # Step 1: Data Preparation\n    run_step(\"copy\")\n    run_step(\"rename\")\n    run_step(\"preprocess\")\n    run_step(\"verify\")\n    run_step(\"display\")\n    run_step(\"split\")\n\n    # Load data\n    train_loader, val_loader, test_loader, class_names = get_dataloaders(SPLIT_BASE)\n\n    # Step 2: Baseline Model \n    print(\"\\n BASE MODEL TRAINING\")\n    base_model = HybridBeitEffNet(dropout=BEST_DROPOUT, num_classes=len(class_names)).to(DEVICE)\n    for p in base_model.beit.parameters():\n        p.requires_grad = False\n    for p in base_model.efficientnet.parameters():\n        p.requires_grad = False\n\n    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n    optimizer = optim.Adam(\n        filter(lambda p: p.requires_grad, base_model.parameters()),\n        lr=BEST_LR,\n        weight_decay=WEIGHT_DECAY\n    )\n\n    train_model(\n        base_model, train_loader, val_loader, criterion, optimizer,\n        epochs=EPOCHS, device=DEVICE, patience=PATIENCE, save_path=\"base_model.pth\"\n    )\n    base_model.load_state_dict(torch.load(\"base_model.pth\", map_location=DEVICE))\n\n    print(\"\\n Base Model Evaluation\")\n    base_metrics = evaluate_model_full(\n        base_model, train_loader, val_loader, test_loader,\n        criterion, class_names, device=DEVICE, model_name=\"Base Model\"\n    )\n    print(base_metrics)\n\n    # Step 3: PSO Optimization \n    print(\"\\n PSO OPTIMIZATION\")\n    best_pso_hps, best_pso_acc, pso_metrics = run_pso_optimization(\n        train_loader, val_loader, test_loader, class_names, DEVICE\n    )\n    print(pso_metrics)\n\n    # Step 4: WOA Optimization\n    print(\"\\n WOA OPTIMIZATION\")\n    best_woa_hps, best_woa_acc, woa_metrics = run_woa_optimization(\n        train_loader, val_loader, test_loader, class_names, DEVICE\n    )\n    print(woa_metrics)\n\n    # Step 5: Final Summary \n    print(\"\\n FINAL SUMMARY\")\n    print(\"Base Model:\", base_metrics)\n    print(\"PSO Model :\", pso_metrics)\n    print(\"WOA Model :\", woa_metrics)\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}